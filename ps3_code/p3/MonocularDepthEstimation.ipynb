{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBciS_TD3JVl"
      },
      "source": [
        "# CS231a PSET 3 Problem 3: Monocular Depth Estimation and Representation Learning\n",
        "\n",
        "In this problem we will train a deep learning model to do monocular depth estimation.\n",
        "\n",
        "**Using a GPU**. Make sure to first change your runtime to use a GPU: click Runtime -> Change runtime type -> Hardware Accelerator -> GPU and your Colab instance will automatically be backed by GPU compute.\n",
        "\n",
        "First, you should upload the files in 'code/p3' directory onto a location of your choosing in Drive and run the following to have access to them. Now, to get the data, run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwQRvE4C3gf5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Enter the foldername in your Drive where you have saved the unzipped\n",
        "# '.py' files from the p3 folder and the \"cs231a-clevr-rgbd.zip\" file\n",
        "# e.g. 'cs231a/pset3/p3'\n",
        "FOLDERNAME = 'cs231a/pset3/p3'\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "%ls .\n",
        "%cd drive/MyDrive\n",
        "%cd $FOLDERNAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: filelock in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/adamsun/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-5.2.0\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IM6gWAZSxae6iVUeEz9BeT73rvr8RwWf\n",
            "From (redirected): https://drive.google.com/uc?id=1IM6gWAZSxae6iVUeEz9BeT73rvr8RwWf&confirm=t&uuid=5c8a49e6-e0f1-4412-83d9-5398ee2fac45\n",
            "To: /Users/adamsun/Downloads/ps3_code/p3/cs231a-clevr-rgbd.zip\n",
            " 16%|██████▏                                | 169M/1.07G [00:12<00:39, 22.6MB/s]"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown 1IM6gWAZSxae6iVUeEz9BeT73rvr8RwWf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cphY32S2uM"
      },
      "source": [
        "If all is set up correctly, you should now get the 1G dataset stored in this Colaborotary runtime. Note that you'll need to redownload this data whenever you reconnect to a fresh runtime!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM_rDf34_xC5"
      },
      "source": [
        "# Checking out the data\n",
        "\n",
        "Let's start by having a look at what's in our CLEVR-D dataset. For that, finish the marked sections in data.py, and then run the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJqDYf2__-HF"
      },
      "outputs": [],
      "source": [
        "import data\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from importlib import reload \n",
        "reload(data)\n",
        "plt.rcParams['figure.figsize'] = [8,10]\n",
        "plt.rcParams['figure.dpi'] = 100 \n",
        "\n",
        "train_data_loader, test_data_loader = data.get_data_loaders(\"cs231a-clevr-rgbd.zip\",\n",
        "                                                is_mono=True,\n",
        "                                                batch_size=16,\n",
        "                                                train_test_split=0.8,\n",
        "                                                pct_dataset=0.2)#0.2 of dataset to keep things fast\n",
        "test_data_iter = iter(test_data_loader)\n",
        "data_sample = next(test_data_iter)\n",
        "print(\"\\nMean, min and max of RGB image - %.3f %.3f %.3f\"%(\n",
        "                                          torch.mean(data_sample['rgb']),\n",
        "                                          torch.min(data_sample['rgb']),\n",
        "                                          torch.max(data_sample['rgb'])))\n",
        "\n",
        "print(\"Mean, min and max of depth image - %.3f %.3f %.3f\\n\"%(\n",
        "                                          torch.mean(data_sample['depth']),\n",
        "                                          torch.min(data_sample['depth']),\n",
        "                                          torch.max(data_sample['depth'])))\n",
        "\n",
        "rgb_tensor_to_image, depth_tensor_to_image = data.get_tensor_to_image_transforms()\n",
        "fig, axs = plt.subplots(3, 2)\n",
        "axs[0,0].set_title('RGB', size='large')\n",
        "axs[0,1].set_title('Depth', size='large')\n",
        "for i in range(3):\n",
        "    axs[i, 0].imshow(rgb_tensor_to_image(data_sample['rgb'][i]))\n",
        "    axs[i, 1].imshow(depth_tensor_to_image(data_sample['depth'][i]), cmap='gray')\n",
        "    axs[i, 0].axis('off')\n",
        "    axs[i, 1].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTffBCOz7qwz"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "Next, we can go ahead and train the model once you complete the appropriate parts of losses.py and training.py. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2a_1JM754no"
      },
      "source": [
        "Before we run training, let's visualize the training progress using [Tensorboard](https://www.tensorflow.org/tensorboard). When you run the following, you should see the scalars tab showing the loss gradually going down once training starts. If you go to the 'images' tab, you can also be able to observe the 'Ours' images getting better over time, with the 'Diff' images showing less disparity from the ground truth over time. Hit the refresh icon on the top right once you get training going in the next bit, and you should be able to see stuff show up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu8yEUgBPpfM"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "%load_ext tensorboard\n",
        "%rm -rf \"/content/drive/MyDrive/$FOLDERNAME/runs/*\"\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/$FOLDERNAME/runs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9veZxXzG7Kch"
      },
      "source": [
        "Let's first initialize the model to pass into the training function and confirm that given an rgb image it outputs a depth image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDgjjdzM7ONX"
      },
      "outputs": [],
      "source": [
        "import model\n",
        "from utils import colorize\n",
        "# if you get a cuda out of memory error here, you need to restart the runtime \n",
        "# and re-run everything\n",
        "dense_depth_model = model.DenseDepth()\n",
        "dense_depth_model = dense_depth_model.to('cuda')\n",
        "sample_image = next(test_data_iter)\n",
        "with torch.no_grad():\n",
        "    model_out = dense_depth_model(sample_image['rgb'].to('cuda')) \n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(rgb_tensor_to_image(sample_image['rgb'][0]))\n",
        "axs[1].imshow(depth_tensor_to_image(model_out[0]),cmap='gray')\n",
        "axs[0].axis('off')\n",
        "axs[1].axis('off')\n",
        "del sample_image\n",
        "del model_out       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb3nUVHlAaBF"
      },
      "source": [
        "We can also make sure the model is correctly loaded onto the GPU and checks its size. Under Memory-Usage, you can see that it takes up approximately 3.5G of this GPU's memory, making it a modestly large model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDTnkggDAftK"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_o6xWVE7V3U"
      },
      "source": [
        "With that done, let's get training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZqgjjPU9HvL"
      },
      "outputs": [],
      "source": [
        "import training \n",
        "import torch \n",
        "# if you get a cuda out of memory error here, you need to restart the runtime \n",
        "# and re-run everything\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "training = reload(training)#reload when debugging to have updated code\n",
        "training.train(5, train_data_loader, test_data_loader, lr=0.0001, model=dense_depth_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnrNcDV7VJ9R"
      },
      "source": [
        "Yay! If you implemented everything correctly, the loss went down and you saw the model work well. We can now again take a look at its output for a given image and see what it does on test set inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv15PJoHy4sw"
      },
      "outputs": [],
      "source": [
        "#we'll iterate to pick a nice set of images\n",
        "for i in range(3): # feel free to change this to see other outputs\n",
        "    sample_image = next(test_data_iter)\n",
        "with torch.no_grad():\n",
        "    model_out = dense_depth_model(sample_image['rgb'].to('cuda')) \n",
        "fig, axs = plt.subplots(3, 3)\n",
        "axs[0,0].set_title('RGB', size='large')\n",
        "axs[0,1].set_title('Predicted Depth', size='large')\n",
        "axs[0,2].set_title('True Depth', size='large')\n",
        "depth_inverse_normalize = data.get_inverse_transforms()[1]\n",
        "for i in range(3):\n",
        "    axs[i, 0].imshow(rgb_tensor_to_image(sample_image['rgb'][i]))\n",
        "    axs[i, 1].imshow(depth_inverse_normalize(model_out[i]).data.cpu().numpy()[0], cmap='gray')\n",
        "    axs[i, 2].imshow(depth_tensor_to_image(sample_image['depth'][i]), cmap='gray')\n",
        "    axs[i, 0].axis('off')\n",
        "    axs[i, 1].axis('off')\n",
        "    axs[i, 2].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPxk3nJXPqjw"
      },
      "source": [
        "We can see that the model is sort of doing the right thing, but because we only trained it on a small subset of the data and for 5 epochs the result is rather blurry. Feel free to try increasing the number of epochs and pct_dataset to see if it improves!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HImhN6DFTkfi"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "That's it, you are done! \n",
        "\n",
        "Credits: this assignment was adapted from [this](https://github.com/pranjaldatta/DenseDepth-Pytorch) code base.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
